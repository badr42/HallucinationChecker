# AI Hallucination and Veracity Checker

This project is an AI-based hallucination checker designed to assess the accuracy of responses generated by an AI model. It combines **Cohere** for response generation and **OpenAI** for veracity checking. This tool identifies hallucinations and confabulations (inaccurate or misleading information) in AI-generated content, making it especially useful in fields where accuracy is essential.

## How It Works

1. **Response Generation**: The tool takes a user prompt and generates a response using Cohere's language model. The response is designed to have a 25% chance of being accurate, while the rest may contain subtle inaccuracies.
2. **Veracity Checking**: The generated response is passed to OpenAIâ€™s `gpt-3.5-turbo` (or `gpt-4-turbo`) model, which evaluates it. The evaluation includes:
   - A **veracity score** (percentage) indicating the overall accuracy of the response.
   - A list of **hallucinated statements** with justifications explaining why each statement is incorrect.

## Features

- **Dual-Model Integration**: Combines Cohere for response generation and OpenAI for accuracy verification.
- **Detailed Analysis**: Outputs only the incorrect portions of the response, making it easier to identify and understand inaccuracies.
- **API Key Security**: API keys are required to be manually entered in the script file, ensuring they're not exposed in version control.

## Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/ai-hallucination-veracity-checker.git
cd ai-hallucination-veracity-checker